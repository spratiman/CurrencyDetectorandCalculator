{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC420-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCDEtS_-6Y2q",
        "colab_type": "code",
        "outputId": "aaf0519c-092d-4023-ffe0-6cc0595c96f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "font = cv2.FONT_HERSHEY_COMPLEX\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-btzroDIGrwG",
        "colab_type": "text"
      },
      "source": [
        "# Step 1 - Identify bank notes in the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf30A4OM2doy",
        "colab_type": "text"
      },
      "source": [
        "## Problem description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDpGOnHA2g2z",
        "colab_type": "text"
      },
      "source": [
        "Given an input image containing one or multiple bank notes of some currency,\n",
        "we need to find the bounding polygons (here, simply rectangles) for each bank note. Individually identified bank notes are then passed to the next step\n",
        "of our image processing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Bc8Akw3EO5",
        "colab_type": "text"
      },
      "source": [
        "## Solution description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZFEL2vk3I5F",
        "colab_type": "text"
      },
      "source": [
        "The color image is passed to a Canny edge detector. We use color image here,\n",
        "because edge detection works better on it, since variations in color form edges that are important to us and would not be captured in a grayscale image.\n",
        "\n",
        "The edge map is then passed to `findContours`, which finds all the closed contours in the edge map. \n",
        "\n",
        "Finally, for each contour we find a bounding rectangle and only keep ones that\n",
        "are at least of size 80x50 (subject to change) and are non-intersecting.\n",
        "If 2 contours are intersecting, we keep the one with a larger area.\n",
        "\n",
        "The selected bounding rectangles are used to crop images of bank notes from\n",
        "the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDiTPiSH43R3",
        "colab_type": "text"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Flbs7T6Yxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change the path name here \n",
        "img_name = \"CAD.png\"\n",
        "# img_name = \"cad-10.jpg\"\n",
        "path_name=\"/content/gdrive/My Drive/Colab Notebooks/Project/\" + img_name\n",
        "img_color = cv2.imread(path_name)\n",
        "img_gray = cv2.imread(path_name, cv2.IMREAD_GRAYSCALE)\n",
        "if img_color is None:\n",
        "  print(\"Could not load the image\")\n",
        "\n",
        "cv2_imshow(img_gray)\n",
        "cv2_imshow(img_color)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYMzyOzcFwNy",
        "colab_type": "text"
      },
      "source": [
        "### Edge detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5S3v7zClatM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detect edges using Canny edge detector\n",
        "edges = cv2.Canny(img_color, 100, 40, L2gradient=True)\n",
        "cv2_imshow(edges)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGkDRiGMF1ks",
        "colab_type": "text"
      },
      "source": [
        "### Contour extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwgcJb_pmWye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find contours in the image of edges\n",
        "# _, threshold = cv2.threshold(edges, 250, 255, cv2.THRESH_BINARY)\n",
        "_, contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqSMqrY3n2Kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For cropping, saving and giving boundary limits to currency\n",
        "\n",
        "def rectanglesIntersection(coords1, size1, coords2, size2):\n",
        "  x = max(coords1[0], coords2[0])\n",
        "  y = max(coords1[1], coords2[1])\n",
        "  w = min(coords1[0] + size1[0], coords2[0] + size2[0]) - x\n",
        "  h = min(coords1[1]+ size1[1], coords2[1] + size2[1]) - y\n",
        "  # print(w, h)\n",
        "  if w <= 0 or h <= 0:\n",
        "    return 0\n",
        "  return w * h\n",
        "\n",
        "def extract_segments(contours, min_width=150, min_height=50, max_width=None, max_height=None):\n",
        "  segments = []\n",
        "  for c in contours:\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    # threshold on the minimum size of the bank note.\n",
        "    # since all images are resized to the same dimensions, should work fine.\n",
        "    if (w > min_width and\n",
        "        h > min_height and \n",
        "        ((max_width is None) or w < max_width) and\n",
        "        ((max_height is None) or h < max_height)):\n",
        "      # out of intersecting contours take the one with the largest area\n",
        "      area = h * w\n",
        "      if len(segments) == 0:\n",
        "        segments.append(((x, y), (w, h)))\n",
        "      add_boundary = True\n",
        "      for i, segment in enumerate(segments):\n",
        "        old_area = segment[1][0] * segment[1][1]\n",
        "        intersection = rectanglesIntersection((x, y), (w, h), segment[0], segment[1])\n",
        "        if intersection > area * 0.1: # more than 10% intersects\n",
        "          add_boundary = False\n",
        "          if old_area < area:\n",
        "            segments[i] = (x, y), (w, h)\n",
        "      if add_boundary:\n",
        "        segments.append(((x, y), (w, h)))\n",
        "  return segments\n",
        "\n",
        "def display_segments(original_img, segments):\n",
        "  for i, segment in enumerate(segments):\n",
        "    (x, y), (w, h) = segment\n",
        "    new_img = original_img[y : y+h, x : x+w]\n",
        "    print(i)\n",
        "    # cv2.imwrite('/content/gdrive/My Drive/Colab Notebooks/Project/segmented-{0}.png'.format(i), new_img)\n",
        "    cv2.imwrite('/content/individual_bills/segmented-{0}.png'.format(i), new_img)\n",
        "    cv2_imshow(new_img)\n",
        "\n",
        "bank_notes = extract_segments(contours)\n",
        "display_segments(img_color, bank_notes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N8pWV2A459R",
        "colab_type": "text"
      },
      "source": [
        "## Results discussion\n",
        "\n",
        "This solution works fairly well, assuming the only big objects in the image are banknotes. Since at this stage we do not classify whether the found object\n",
        "is a banknote or not, any object that's large enough would be selected.\n",
        "That's fine, however, since our proposal does not suggest we can deal with a more complex case. And if needed, we could add it later.\n",
        "\n",
        "The biggest downfall of this solution is that oftentimes it doesn't include\n",
        "the full image of the bank note, because the contour breaks around the transparent strip on the right (for CAD at least). The good thing is that most of the important information is on the left of that strip (such as denomination, country of origin, maple leaf), so it shouldn't affect the rest of our pipeline that much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Yr8aVWGyc4",
        "colab_type": "text"
      },
      "source": [
        "# Step 2 - Segment bank notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOWfWXowH93H",
        "colab_type": "text"
      },
      "source": [
        "## Problem description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXKy5d6HT0i",
        "colab_type": "text"
      },
      "source": [
        "Image segmentation on the bank note should identify areas of interest.\n",
        "For example, the segmentation mask would contain areas with denomination of the currency, the name of the country of origin, country-specific features, such as flags. This would allow us to apply classification of areas of interest in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI68q8DaICMM",
        "colab_type": "text"
      },
      "source": [
        "## Implementation plans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o_Y5yfnIK4V",
        "colab_type": "text"
      },
      "source": [
        "We already know that the task of image segmentation is best accomplished by a convolutional neural network with encoder/decoder architecture, such as UNet.\n",
        "So for this part of the project we can just reuse UNet from our A3. However, the main challenge here is that we have little to no data of images of currency.\n",
        "We have found no dataset that contains images of currency and corresponding segmentation masks with areas of interest. Any training/testing data we would need to create ourselves. It is possible to manually create a small dataset,\n",
        "but it clearly would not be good enough to get high accuracy on the test set.\n",
        "\n",
        "One approach I want to try here is transfer learning, just like in A3. Train UNet on a large dataset of different data and then use the pretrained model\n",
        "to improve performance on our small currency data set. The difference from A3\n",
        "is that there transfer dataset was very similar to the original one (i.e. it also contained images of cats), which is not the case here. So we probably need a different approach to transfer learning. For example, treat the last layer of that UNet as a feature input that it is input to our currency segmentator.\n",
        "\n",
        "Well, in any case, our first order of business should be to obtain a currency dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCW3FRCkLTRw",
        "colab_type": "text"
      },
      "source": [
        "## UNet implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEhMoSkTbePa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import cv2 as cv\n",
        "import glob\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def diceCoef(mask, predicted_mask, smooth=10 ** (-7)):\n",
        "    intersection = K.sum(K.abs(mask * predicted_mask), axis=[1,2,3])\n",
        "    union = K.sum(K.abs(mask), axis=[1, 2, 3]) + K.sum(K.abs(predicted_mask), axis=[1, 2, 3])\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "    return dice\n",
        "\n",
        "class UNet:\n",
        "    def __init__(self, loss=\"binary_crossentropy\"):\n",
        "        self.test_input = None\n",
        "        self.train_input = None\n",
        "        self.test_seg = None\n",
        "        self.train_seg = None\n",
        "        self.train_n = 0\n",
        "        self.test_n = 0\n",
        "        self.model = None\n",
        "        self.loss = loss\n",
        "    \n",
        "    def buildModel(self):\n",
        "        # contracting path\n",
        "        inputs = keras.Input(shape=(256, 256, 1))\n",
        "        conv64_1 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\",\n",
        "                                 kernel_initializer=\"he_normal\")(inputs)\n",
        "        conv64_2 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\",\n",
        "                                 kernel_initializer=\"he_normal\")(conv64_1)\n",
        "        dropout64 = layers.Dropout(0.2)(conv64_2)\n",
        "        maxpool1 = layers.MaxPool2D(pool_size=(2, 2), strides=2,\n",
        "                                    padding=\"same\")(dropout64) # 64 x 64\n",
        "        conv128_1 = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\",\n",
        "                                  kernel_initializer=\"he_normal\")(maxpool1)\n",
        "        conv128_2 = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\",\n",
        "                                  kernel_initializer=\"he_normal\")(conv128_1)\n",
        "        dropout128 = layers.Dropout(0.2)(conv128_2)\n",
        "        maxpool2 = layers.MaxPool2D(pool_size=(2, 2), strides=2,\n",
        "                                    padding=\"same\")(dropout128) # 32 x 32\n",
        "        conv256_1 = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\",\n",
        "                                  kernel_initializer=\"he_normal\")(maxpool2)\n",
        "        conv256_2 = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\",\n",
        "                                  kernel_initializer=\"he_normal\")(conv256_1)\n",
        "        dropout256 = layers.Dropout(0.2)(conv256_2)\n",
        "\n",
        "        maxpool3 = layers.MaxPool2D(pool_size=(2, 2), strides=2,\n",
        "                                    padding=\"same\")(dropout256) # 16 x 16\n",
        "        conv512_1 = layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\",\n",
        "                                  kernel_initializer=\"he_normal\")(maxpool3)\n",
        "        conv512_2 = layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\",\n",
        "                                  kernel_initializer=\"he_normal\")(conv512_1)\n",
        "        dropout512 = layers.Dropout(0.2)(conv512_2)\n",
        "        upsample2 = layers.UpSampling2D(size=(2, 2))(dropout512) # 32 x 32\n",
        "        upconv2 = layers.Conv2DTranspose(256, 2, padding=\"same\",\n",
        "                                         activation=\"relu\",\n",
        "                                         kernel_initializer=\"he_normal\")(upsample2)\n",
        "        concat2 = layers.Concatenate()([conv256_2, upconv2])\n",
        "        conv256up_1 = layers.Conv2DTranspose(128, 3, padding=\"same\",\n",
        "                                             activation=\"relu\",\n",
        "                                             kernel_initializer=\"he_normal\")(concat2)\n",
        "        conv256up_2 = layers.Conv2DTranspose(128, 3, padding=\"same\",\n",
        "                                             activation=\"relu\",\n",
        "                                             kernel_initializer=\"he_normal\")(conv256up_1)\n",
        "        dropout256up = layers.Dropout(0.2)(conv256up_2)\n",
        "\n",
        "        upsample3 = layers.UpSampling2D(size=(2, 2))(dropout256up) # 64 x 64\n",
        "        upconv3 = layers.Conv2DTranspose(128, 2, padding=\"same\",\n",
        "                                         activation=\"relu\",\n",
        "                                         kernel_initializer=\"he_normal\")(upsample3)\n",
        "        concat3 = layers.Concatenate()([conv128_2, upconv3])\n",
        "        conv128up_1 = layers.Conv2DTranspose(128, 3, padding=\"same\",\n",
        "                                             activation=\"relu\",\n",
        "                                             kernel_initializer=\"he_normal\")(concat3)\n",
        "        conv128up_2 = layers.Conv2DTranspose(128, 3, padding=\"same\",\n",
        "                                             activation=\"relu\",\n",
        "                                             kernel_initializer=\"he_normal\")(conv128up_1)\n",
        "        dropout128up = layers.Dropout(0.2)(conv128up_2)\n",
        "        upsample4 = layers.UpSampling2D(size=(2, 2))(dropout128up) # 128 x 128\n",
        "        upconv4 = layers.Conv2DTranspose(64, 2, padding=\"same\",\n",
        "                                         activation=\"relu\",\n",
        "                                         kernel_initializer=\"he_normal\")(upsample4)\n",
        "        concat4 = layers.Concatenate()([conv64_2, upconv4])\n",
        "        conv64up_1 = layers.Conv2DTranspose(64, 3, padding=\"same\",\n",
        "                                            activation=\"relu\",\n",
        "                                            kernel_initializer=\"he_normal\")(concat4)\n",
        "        conv64up_2 = layers.Conv2DTranspose(64, 3, padding=\"same\",\n",
        "                                            activation=\"relu\",\n",
        "                                            kernel_initializer=\"he_normal\")(conv64up_1)\n",
        "        dropout64up = layers.Dropout(0.2)(conv64up_2)\n",
        "        conv64up_3 = layers.Conv2DTranspose(2, 3, padding=\"same\",\n",
        "                                            kernel_initializer=\"he_normal\")(dropout64up)\n",
        "        \n",
        "        outputs = layers.Conv2DTranspose(\n",
        "            filters=1,\n",
        "            kernel_size=1,\n",
        "            padding=\"same\",\n",
        "            activation=\"sigmoid\",\n",
        "            kernel_initializer=\"he_normal\"\n",
        "       )(conv64up_3)\n",
        "\n",
        "\n",
        "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        \n",
        "        self.model.compile(optimizer=optimizers.Adam(),\n",
        "              loss=self.loss,\n",
        "              metrics=['accuracy', diceCoef])\n",
        "        print(\"UNet model is complete\")\n",
        "\n",
        "    def train(self, epochs, batch_size=10, validation_split=0.15):\n",
        "        print(\"Starting training...\")\n",
        "        self.model.fit(\n",
        "            self.train_input,\n",
        "            self.train_seg,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=validation_split)\n",
        "\n",
        "    \n",
        "    def evaluate(self):\n",
        "        print(\"Evaluating segmentation model...\")\n",
        "        self.model.evaluate(self.test_input, self.test_seg, verbose=2)\n",
        "\n",
        "    def predict(self, imgs):\n",
        "        predictions = self.model.predict(imgs)\n",
        "        return predictions\n",
        "    \n",
        "    def saveWeights(self, name):\n",
        "        self.model.save_weights(name)\n",
        "    \n",
        "    def loadWeights(self, name):\n",
        "        self.model.load_weights(name)\n",
        "    \n",
        "    def plotPrediction(self, prediction, truth, img):\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        fig.add_subplot(1, 3, 1)\n",
        "        plt.imshow(prediction[...,0], cmap=\"gray\")\n",
        "        fig.add_subplot(1, 3, 2)\n",
        "        plt.imshow(truth[..., 0], cmap=\"gray\")\n",
        "\n",
        "        fig.add_subplot(1, 3, 3)\n",
        "        plt.imshow(img[..., 0], cmap=\"gray\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def loadData(self, trainInputDirPath, trainSegDirPath, testInputDirPath, testSegDirPath, trainAugInput=None, trainAugSeg=None):\n",
        "        self.train_input = self.__loadDataFromDir(trainInputDirPath)\n",
        "        self.test_input = self.__loadDataFromDir(testInputDirPath)\n",
        "        self.train_seg = self.__loadDataFromDir(trainSegDirPath)\n",
        "        self.test_seg = self.__loadDataFromDir(testSegDirPath)\n",
        "        if trainAugInput and trainAugSeg:\n",
        "            self.train_input = np.vstack((self.train_input, self.__loadDataFromDir(trainAugInput)))\n",
        "            self.train_seg = np.vstack((self.train_seg, self.__loadDataFromDir(trainAugSeg)))\n",
        "        self.train_n = self.train_input.shape[0]\n",
        "        self.test_n = self.test_input.shape[0]\n",
        "\n",
        "        print(\"Data is loaded:\")\n",
        "        print(\"Number of training images: {0}\".format(self.train_n))\n",
        "        print(\"Number of testing images: {0}\".format(self.test_n))\n",
        "        print(\"Training input set shape: {0}\".format(self.train_input.shape))\n",
        "        print(\"Training segmentation set shape: {0}\".format(self.train_seg.shape))\n",
        "        print(\"Testing input set shape: {0}\".format(self.test_input.shape))\n",
        "        print(\"Testing segmentation set shape: {0}\".format(self.test_seg.shape))\n",
        "\n",
        "    def loadDataAndSplit(self, inputDirPath, segDirPath):\n",
        "        # load data and split it into train and test sets\n",
        "        inputs = self.__loadDataFromDir(inputDirPath)\n",
        "        masks = self.__loadDataFromDir(segDirPath)\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        self.train_input, self.test_input, \\\n",
        "        self.train_seg, self.test_seg = \\\n",
        "        train_test_split(inputs, masks, test_size=0.2, random_state=42)\n",
        "    \n",
        "    def __loadDataFromDir(self, dirPath):\n",
        "        imgs = []\n",
        "        for i, imgName in enumerate(sorted(glob.glob(dirPath + \"/*\"))):\n",
        "            img = cv.imread(imgName)\n",
        "            img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
        "            img = cv.resize(img, (256, 256)) / 255.0\n",
        "            img = np.expand_dims(img, axis=2).astype(np.float32)\n",
        "            imgs.append(img)\n",
        "        \n",
        "        imgsMatrix = np.array(imgs)\n",
        "        return imgsMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSv6Wtkm6obi",
        "colab_type": "text"
      },
      "source": [
        "## Training UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuC3nl20Z1nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"/content/gdrive/My Drive/Colab Notebooks/Project/oxford_data\" ./\n",
        "# !unzip \"/content/oxford_data-20191107T212728Z-001.zip\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1zhVhHq6sR4",
        "colab_type": "code",
        "outputId": "3d1451b6-e8f9-4da7-b66e-8416f4d216e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "transferUnet = UNet(loss=\"binary_crossentropy\")\n",
        "transferUnet.loadData(\n",
        "        \"./oxford_data/Train/input\",\n",
        "        \"./oxford_data/Train/output\",\n",
        "        \"./oxford_data/Test/input\",\n",
        "        \"./oxford_data/Test/output\")\n",
        "transferUnet.buildModel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data is loaded:\n",
            "Number of training images: 0\n",
            "Number of testing images: 0\n",
            "Training input set shape: (0,)\n",
            "Training segmentation set shape: (0,)\n",
            "Testing input set shape: (0,)\n",
            "Testing segmentation set shape: (0,)\n",
            "UNet model is complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSwSk8Qa9SKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_epochs = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjZRvFBi9IAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 15\n",
        "total_epochs += epochs\n",
        "transferUnet.train(epochs, 32)\n",
        "transferUnet.evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVfZGNom9aGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transferUnet.saveWeights(\"/content/gdrive/My Drive/Colab Notebooks/Project/oxford_unet_256_epochs_{0}.h5\".format(total_epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4ASFhF-TlIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTestPrediction(unet, i=0):\n",
        "    img = unet.test_input[i]\n",
        "    predictions = unet.predict(np.array([img]))\n",
        "    unet.plotPrediction(predictions[0], unet.test_seg[i], unet.test_input[i])\n",
        "\n",
        "def plotTrainPrediction(unet, i=0):\n",
        "    img = unet.train_input[i]\n",
        "    predictions = unet.predict(np.array([img]))\n",
        "    print(crossEntropy(predictions, np.array([unet.train_seg[i]])))\n",
        "    unet.plotPrediction(predictions[0], unet.train_seg[i], unet.train_input[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SHLO3M-hsWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotTestPrediction(transferUnet, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mUqPrXmBeQ8",
        "colab_type": "text"
      },
      "source": [
        "## Refine UNet on currency data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8EKUuP08f2u",
        "colab_type": "text"
      },
      "source": [
        "We have a very small data set of bills and their segmentation masks,\n",
        "so the results that we get are probably overfitted and suboptimal.\n",
        "One thing that we've done to improve it, is to augment our data set. \n",
        "We've randomly rotated every image/mask pair in the dataset.\n",
        "Other transformations do not really make sense, as translating/zooming in might\n",
        "make the segment we want move out of the view entirely.\n",
        "\n",
        "One possibly transformation that we could do is a projective transform. \n",
        "However, I'm not sure how to implement that right now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rx9IMsscVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp -r \"/content/gdrive/My Drive/Colab Notebooks/Project/currency_data\" ./\n",
        "!cp \"/content/gdrive/My Drive/Colab Notebooks/Project/data.zip\" ./\n",
        "!unzip \"/content/data.zip\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMor0zU4Gr19",
        "colab_type": "text"
      },
      "source": [
        "We have a very small dataset, so we augmented\n",
        "it by randomly rotating every image a bunch of times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00Lgbn-GoAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DataAugmentor:\n",
        "    def __init__(self, inputDirPath, maskDirPath):\n",
        "        # read all images\n",
        "        imgs = []\n",
        "        masks = []\n",
        "        for imgName in glob.glob(inputDirPath + \"/*\"):\n",
        "            img = cv.imread(imgName)\n",
        "            img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
        "            img = cv.resize(img, (128, 128)) / 255.0\n",
        "            imgs.append(img)\n",
        "        self.imgs = np.array(imgs)\n",
        "        for imgName in glob.glob(maskDirPath + \"/*\"):\n",
        "            img = cv.imread(imgName)\n",
        "            img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
        "            img = cv.resize(img, (128, 128)) / 255.0\n",
        "            masks.append(img)\n",
        "        self.masks = np.array(masks)\n",
        "        \n",
        "        self.augmentedImgs = []\n",
        "        self.augmentedMasks = []\n",
        "    \n",
        "    def augmentAll(self, n=1):\n",
        "        for epoch in range(n):\n",
        "            # randomly rotate every image n number of times\n",
        "            for i in range(len(self.imgs)):\n",
        "                img = self.imgs[i]\n",
        "                mask = self.masks[i]\n",
        "                img, mask = self.rotateRandom(img, mask)\n",
        "                self.augmentedImgs.append(img)\n",
        "                self.augmentedMasks.append(mask)\n",
        "        self.augmentedImgs = np.array(self.augmentedImgs)\n",
        "        self.augmentedMasks = np.array(self.augmentedMasks)\n",
        "    \n",
        "    def saveAugmented(self, inputDirPath, maskDirPath):\n",
        "        print(\"Saving {0} augmented images\".format(len(self.augmentedImgs)))\n",
        "        for i in range(len(self.augmentedImgs)):\n",
        "            fileName = \"{0}/augmented-{1}.jpg\".format(inputDirPath, i)\n",
        "            cv.imwrite(fileName, self.augmentedImgs[i] * 255)\n",
        "        for i in range(len(self.augmentedMasks)):\n",
        "            fileName = \"{0}/augmented-{1}.jpg\".format(maskDirPath, i)\n",
        "            cv.imwrite(fileName, self.augmentedMasks[i] * 255)\n",
        "\n",
        "    def rotateRandom(self, img, mask):\n",
        "        rotation = np.random.randint(1, 36) * 10\n",
        "\n",
        "        rows, cols = img.shape[:2]\n",
        "        M = cv.getRotationMatrix2D((cols / 2,rows / 2), rotation ,1)\n",
        "        img = cv.warpAffine(img, M, (cols,rows))\n",
        "        mask = cv.warpAffine(mask, M, (cols, rows))\n",
        "        return img, mask\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    augmentor = DataAugmentor(\"./inputs\", \"./masks\")\n",
        "    augmentor.augmentAll(10)\n",
        "    augmentor.saveAugmented(\"./inputs\", \"./masks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osfYBobltw2b",
        "colab_type": "code",
        "outputId": "6226cc3f-b259-4738-ca1b-8ecea4f0461a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "currencyUnet = UNet(loss=\"binary_crossentropy\")\n",
        "currencyUnet.loadDataAndSplit(\n",
        "        \"./currency_data/inputs\",\n",
        "        \"./currency_data/masks\")\n",
        "currencyUnet.buildModel()\n",
        "currencyUnet.loadWeights(\"/content/gdrive/My Drive/Colab Notebooks/Project/oxford_unet_epochs_35.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNet model is complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BjhvEuxui-0",
        "colab_type": "code",
        "outputId": "227f9f72-d527-49d2-9ad4-e029c2d305c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "currencyUnet.train(10, 8, 0)\n",
        "currencyUnet.evaluate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Train on 96 samples\n",
            "Epoch 1/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0366 - accuracy: 0.9504 - diceCoef: 0.8124\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0304 - accuracy: 0.9519 - diceCoef: 0.8495\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0263 - accuracy: 0.9526 - diceCoef: 0.8656\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0245 - accuracy: 0.9528 - diceCoef: 0.8744\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0226 - accuracy: 0.9532 - diceCoef: 0.8834\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0209 - accuracy: 0.9535 - diceCoef: 0.8938\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0205 - accuracy: 0.9535 - diceCoef: 0.8986\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0182 - accuracy: 0.9539 - diceCoef: 0.9051\n",
            "Epoch 9/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0184 - accuracy: 0.9538 - diceCoef: 0.9042\n",
            "Epoch 10/10\n",
            "96/96 [==============================] - 3s 29ms/sample - loss: 0.0187 - accuracy: 0.9539 - diceCoef: 0.9058\n",
            "Evaluating segmentation model...\n",
            "25/1 - 0s - loss: 0.1649 - accuracy: 0.9354 - diceCoef: 0.5617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi6exgRHBlPo",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix21M5ZWs6pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "bills = []\n",
        "for i, imgName in enumerate(sorted(glob.glob(\"/content/individual_bills/*\"))):\n",
        "  billImg = cv2.imread(imgName)\n",
        "  billImg = cv2.cvtColor(billImg, cv2.COLOR_RGB2GRAY)\n",
        "  billImg = np.expand_dims(cv2.resize(billImg, (256, 256)) / 255.0, 2)\n",
        "\n",
        "  bills.append(billImg)\n",
        "\n",
        "bills = np.array(bills)\n",
        "print(\"Predicting masks for {0} bills\".format(len(bills)))\n",
        "predictions = currencyUnet.predict(bills)\n",
        "for i, prediction in enumerate(predictions):\n",
        "  prediction = prediction[..., 0]\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.add_subplot(1, 2, 1)\n",
        "  plt.imshow(prediction, cmap=\"gray\")\n",
        "  fig.add_subplot(1, 2, 2)\n",
        "  plt.imshow(bills[i][..., 0], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YY2T3LQBpnU",
        "colab_type": "text"
      },
      "source": [
        "# Step 3 - Extract areas of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPm66VlJXFT6",
        "colab_type": "text"
      },
      "source": [
        "## Extract segment with denomination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCdjTTurW1T3",
        "colab_type": "text"
      },
      "source": [
        "Threshold the segmentation mask to obtain strong boundaries and then\n",
        "find contours in the resulting mask. Extract bounding rectangles for values on bills."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16IXDkG_z1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, prediction in enumerate(predictions):\n",
        "  predicted_mask = (prediction[...,0] * 255).astype(np.uint8)\n",
        "  _, thresholded = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
        "  _, contours_bill, _ = cv2.findContours(thresholded, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  original_contour = bills[i][..., 0] * 255\n",
        "  original = bills[i][..., 0] * 255\n",
        "  segments = []\n",
        "  for j, cnt in enumerate(contours_bill):\n",
        "      approx = cv2.approxPolyDP(cnt, 0.01 * cv2.arcLength(cnt, True), True)\n",
        "      cv2.drawContours(original_contour, [approx], 0, (0), 1)\n",
        "      x = approx.ravel()[0]\n",
        "      y = approx.ravel()[1]\n",
        "      x, y, w, h = cv2.boundingRect(cnt)\n",
        "      segment = original[y : y + h, x : x + w]\n",
        "      segments.append(segment)\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  # fig.suptitle(\"Thresholded mask for bill {0}\".format(i))\n",
        "  fig.add_subplot(1, 3, 1)\n",
        "  plt.imshow(thresholded, cmap=\"gray\")\n",
        "  fig.add_subplot(1, 3, 2)\n",
        "  plt.imshow(bills[i][..., 0], cmap=\"gray\")\n",
        "  fig.add_subplot(1, 3, 3)\n",
        "  plt.imshow(original_contour, cmap=\"gray\") \n",
        "\n",
        "  areas = extract_segments(contours_bill, 10, 80, 120, 120)\n",
        "  for j, area in enumerate(areas):\n",
        "    (x, y), (w, h) = area\n",
        "    segment = original[y : y+h, x : x+w]\n",
        "    cv2.imwrite(\"./segmented_bills/segment-{0}-{1}.jpg\".format(i, j), segment)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of5_KvCknIx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_images = []\n",
        "for i, segmentName in enumerate(sorted(glob.glob(\"/content/segmented_bills/*\"))):\n",
        "  value = cv2.imread(segmentName, cv2.CV_8UC1)\n",
        "  mean = np.mean(value)\n",
        "  ret, thresh_img = cv2.threshold(value, mean * 0.8, 255, cv2.THRESH_BINARY)\n",
        "  cv2_imshow(thresh_img)\n",
        "  value_images.append(thresh_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3rAE-YPXL6L",
        "colab_type": "text"
      },
      "source": [
        "## Extract digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DDOtk09lzKC",
        "colab_type": "text"
      },
      "source": [
        "Extract digits from the images of bill values,\n",
        "and preprocess each digit into a 28x28 image,\n",
        "which can be fed directly into the neural network\n",
        "in the next step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-7gSNuKps0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imutils.perspective import four_point_transform\n",
        "from imutils import contours\n",
        "import imutils\n",
        "\n",
        "bills_to_digits = {}\n",
        "for j, value_img in enumerate(value_images):\n",
        "\t# find contours in the image, then initialize the\n",
        "\t# digit contours lists\n",
        "\tedges = cv2.Canny(value_img, 100, 200)\n",
        "\tcnts = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\tcnts = imutils.grab_contours(cnts)\n",
        " \n",
        "\tdigitCnts = extract_segments(cnts, 10, 40)\n",
        "\t# display_segments(value_img, digitsCnts)\n",
        "\n",
        "\t# # loop over each of the digits\n",
        "\tbills_to_digits[j] = []\n",
        "\tfor i, c in enumerate(digitCnts):\n",
        "\t\t# extract the digit ROI\n",
        "\t\t(x, y), (w, h) = c[0], c[1]\n",
        "\t\troi = value_img[y:y + h, x:x + w]\n",
        "\t\tcv2_imshow(roi)\n",
        "\t\t# convert every digit image into a \n",
        "\t\t# 28x28 image, while retaining the original\n",
        "\t\t# ratios.\n",
        "\t\t# This is done to improve digit recognition,\n",
        "\t\t# as modifying ratios often leads to incorrect\n",
        "\t\t# predictions.\n",
        "\t\timg_square = np.zeros((28, 28))\n",
        "\t\timg_square[:, :] = 255\n",
        "\t\t# make digit slightly wider than original\n",
        "\t\tratio = 1.5 * roi.shape[1] / roi.shape[0]\n",
        "\t\tscaled_width = int(ratio * 28)\n",
        "\t\tleftOffset = (28 - scaled_width) // 2\n",
        "\t\trightOffset = leftOffset\n",
        "\t\tif scaled_width % 2 != 0:\n",
        "\t\t\trightOffset += 1\n",
        "\t\troi = cv2.resize(roi, (scaled_width, 20), interpolation=cv2.INTER_LINEAR)\n",
        "\t\timg_square[3: 23, leftOffset : -rightOffset] = roi\n",
        "\t\t_, img_square = cv2.threshold(img_square, 150, 255, cv2.THRESH_BINARY_INV)\n",
        "\t\t# img_square[3: 23, 3 : 23] = img\n",
        "\n",
        "\t\t\n",
        "\t\tcv2.imwrite(\"./digit-{0}.png\".format(i), img_square)\n",
        "\t\tcv2_imshow(img_square)\n",
        "\t\t# add x coordinate to digit, so that we can\n",
        "\t\t# tell which digit comes first\n",
        "\t\tbills_to_digits[j].append((x, img_square))\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT6O3m8YXPBm",
        "colab_type": "text"
      },
      "source": [
        "## Digit classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQiIMu5lyKDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the final model to file\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Reference: https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "  # load dataset\n",
        "  (trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "  # reshape dataset to have a single channel\n",
        "  trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "  testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "  \n",
        "  # one hot encode target values\n",
        "  trainY = to_categorical(trainY, 10)\n",
        "  testY = to_categorical(testY, 10)\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "# scale pixels\n",
        "def pre_process(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "# define cnn model\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.25))\n",
        "  model.add(layers.Flatten(), input_shape=(28, 28, 1))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# run the test to evaluate the model\n",
        "def test():\n",
        "    # load dataset\n",
        "    trainX, trainY, testX, testY = load_dataset()\n",
        "    cv2_imshow(testX[1][..., 0])\n",
        "    # prepare pixel data\n",
        "    trainX, testX = pre_process(trainX, testX)\n",
        "    # define model\n",
        "    model = define_model()\n",
        "    # fit model\n",
        "    model.fit(trainX, trainY, epochs=10, batch_size=128, verbose=1)\n",
        "    # save model\n",
        "    model.save('final_model.h5')\n",
        "    # load model\n",
        "    # model = load_model('final_model.h5')\n",
        "    # _, acc = model.evaluate(testX, testY, verbose=0)\n",
        "    # print('> %.3f' % (acc * 100.0))\n",
        "\t\t\t\n",
        "# entry point, run the test harness\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rESVWrYDcI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and prepare the image\n",
        "def load_image(filename):\n",
        "\t# load the image\n",
        "\timg = cv2.imread(filename, cv2.COLOR_BGR2GRAY)\n",
        "\t# reshape into a single sample with 1 channel\n",
        "\timg_square = img\n",
        "\tcv2_imshow(img_square)\n",
        "\timg_square = np.expand_dims(img_square, 0)\n",
        "\timg_square = np.expand_dims(img_square, 3)\n",
        "\t# prepare pixel data\n",
        "\timg_square = img_square.astype('float32')\n",
        "\timg_square = img_square / 255.0\n",
        "\treturn img_square\n",
        "\n",
        "def predict_value(image):\n",
        "\t# load the image\n",
        "\timg = np.expand_dims(image, 0)\n",
        "\timg = np.expand_dims(img, 3)\n",
        "\t# prepare pixel data\n",
        "\timg = img.astype('float32')\n",
        "\timg = img / 255.0\n",
        "\t# img = load_image(image)\n",
        "\t# load model\n",
        "\tmodel = load_model('final_model.h5')\n",
        "\t# predict the class\n",
        "\tdigit = model.predict_classes(img)\n",
        "\tprint(digit[0])\n",
        "\treturn str(digit[0])\n",
        "\n",
        "# entry point, run the example\n",
        "bill_values = []\n",
        "for index, digitImgs in bills_to_digits.items():\n",
        "\tvalue = \"\"\n",
        "\tprint(\"Predicting value of bill\", index)\n",
        "\tfor digitOrder, digitImg in sorted(digitImgs, key = lambda pair: pair[0]):\n",
        "\t\tcv2_imshow(digitImg)\n",
        "\t\tvalue += predict_value(digitImg)\n",
        "\tvalue = int(value)\n",
        "\tprint(\"Predicted value is\", value)\n",
        "\tbill_values.append(value)\n",
        "print(\"Bill values \"+ str(bill_values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSeTDAv6XWCX",
        "colab_type": "text"
      },
      "source": [
        "# Step 4 - Currency type classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEZybkzJZe1h",
        "colab_type": "text"
      },
      "source": [
        "Determine whether the bill is CAD or USD by doing template matching\n",
        "against maple leaf and federeal reserve stamp. If maple leaf results in a better\n",
        "match, classify as CAD, if federal reserve stamp results in a better match,\n",
        "classify as USD. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqWyasLE7hdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imutils\n",
        "\n",
        "# Load the first bill image and convert to grayscale\n",
        "image = cv2.imread('/content/individual_bills/segmented-0.png')\n",
        "# image = cv2.imread('/content/gdrive/My Drive/Colab Notebooks/Project/usd-5-front.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "cv2_imshow(gray)\n",
        "\n",
        "template_cad = cv2.imread('/content/gdrive/My Drive/Colab Notebooks/Project/tcad-20-front-2.jpg')\n",
        "template_usd = cv2.imread('/content/gdrive/My Drive/Colab Notebooks/Project/tusd-100-front-2.jpg')\n",
        "template_usd = cv2.resize(template_usd, (template_cad.shape[1], template_cad.shape[0]))\n",
        "\n",
        "def gradient(img):\n",
        "  lap = cv2.Laplacian(img, cv2.CV_8U)\n",
        "  _, thresh = cv2.threshold(lap, 100, 255, cv2.THRESH_BINARY)\n",
        "  return thresh\n",
        "\n",
        "def matchTemplateAtScale(image, template):\n",
        "  template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
        "  template = imutils.resize(template, width = int(template.shape[1] * 0.5))\n",
        "  template = cv2.Canny(template, 100, 200)\n",
        "  # template = gradient(template)\n",
        "\n",
        "  (tH, tW) = template.shape[:2]\n",
        "  cv2_imshow(template)\n",
        "\n",
        "  print(\"Template size\", template.shape)\n",
        "  print(\"Image size\", image.shape)\n",
        "\n",
        "  # loop over the scales of the image\n",
        "  orig_width = gray.shape[1]\n",
        "  globalMinVal, globalMinLoc = np.float32(\"-inf\"), None\n",
        "  optimalSize = gray.shape[1], gray.shape[0]\n",
        "  optimalR = 0\n",
        "  optimalResized = None\n",
        "  for scale in np.linspace(0.2, 2, 50)[::-1]:\n",
        "    # resize the image according to the scale, and keep track\n",
        "    # of the ratio of the resizing\n",
        "    resized = imutils.resize(gray, width = int(gray.shape[1] * scale))\n",
        "    if resized.shape[1] < template.shape[1] or resized.shape[0] < template.shape[0]:\n",
        "      # scale is too small\n",
        "      continue\n",
        "    r = orig_width / float(resized.shape[1])\n",
        "    # detect edges in the resized, grayscale image and apply template\n",
        "    # matching to find the template in the image\n",
        "    edges = cv2.Canny(resized, 100, 200)\n",
        "    result = cv2.matchTemplate(edges, template, cv2.TM_CCOEFF)\n",
        "    # cv2_imshow(thresh)\n",
        "    (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(result)\n",
        "    if maxVal > globalMinVal:\n",
        "      globalMinVal = maxVal\n",
        "      globalMinLoc = maxLoc\n",
        "      optimalSize = resized.shape[1], resized.shape[0]\n",
        "      optimalR = r\n",
        "      optimalResized = resized\n",
        "    (startX, startY) = (int(maxLoc[0]), int(maxLoc[1]))\n",
        "    (endX, endY) = (startX + tW, startY + tH)\n",
        "\n",
        "  print(\"Highest score:\", globalMinVal)\n",
        "\n",
        "  (startX, startY) = (int(globalMinLoc[0]),\n",
        "                      int(globalMinLoc[1]))\n",
        "  (endX, endY) = (int((globalMinLoc[0] + tW)),\n",
        "                  int((globalMinLoc[1] + tH)))\n",
        "\n",
        "  # draw a bounding box around the detected result and display the image\n",
        "  cv2.rectangle(optimalResized, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
        "  cv2_imshow(optimalResized)\n",
        "  return globalMinVal\n",
        "\n",
        "cad_score = matchTemplateAtScale(image, template_cad)\n",
        "usd_score = matchTemplateAtScale(image, template_usd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUBw0c2rlqXv",
        "colab_type": "text"
      },
      "source": [
        "# Final aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFk5Wj5zlt7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_value = sum(bill_values)\n",
        "if cad_score > usd_score:\n",
        "  denom = \"CAD\"\n",
        "else:\n",
        "  denom = \"USD\"\n",
        "\n",
        "print(\"The image contains ${0} {1}\".format(total_value, denom))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}